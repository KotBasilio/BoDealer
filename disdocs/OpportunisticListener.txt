TAGs: GriffinsClubInternals, DesignDoc

# ğŸ¦¦â†’ğŸ¦‰ Flow Design Doc (v0.1)
_Oscar the Owl as an Opportunistic Listener / Kibitzer_ ğŸ˜¹ğŸ¦‰  
Scope: this doc is about **Walrus Worker (ğŸ¦¦)** streaming human-readable output and **Oscar Owl (ğŸ¦‰)** ingesting it into a structured task model.

This is intentionally â€œlow-invasiveness firstâ€: ğŸ¦¦ may mumble in his own human-readable language; ğŸ¦‰ should opportunistically recognize markers already present and turn them into progress + final results for UI.

---

## 0) Glossary / Roles
- **ğŸ“±ğŸ–¥ï¸ğŸ’» UI clients**: create tasks, poll status, ACK/cancel.
- **ğŸ¦‰ Oscar**: HTTP server + TaskRegistry + process supervisor + listener.
- **ğŸ¦¦ Walter/Walrus**: per-task worker process; may do heavy sampling + DD solving.
- **TaskRegistry**: Oscarâ€™s single source of truth for task state + last seq + logs.

---

## 1) Goals
1) Make ğŸ¦‰ reliably extract:
   - task metadata (task type, question, fixed hand, scorers)
   - filter/search summary (iterations, accepted boards, skipped, pick rate, time/mem)
   - solve progress (processed boards, chance, comparison %, etc.)
   - final answer (split tables, verdict, key metrics)
2) Handle failures gracefully:
   - â€œsemantic refusalâ€ (bad input, contradictory constraints)
   - technical failures (spawn, crash, stall)
3) Provide UI-friendly state:
   - STARTED / IN_PROGRESS / DONE / FAILED / CANCELED
   - incremental updates via `GET /tasks/:id`
4) Keep it testable: build features in an order where each step is verifiable.

---

## 2) Non-Goals (for v0.1)
- Exactly-once delivery end-to-end
- Distributed scheduling / multi-host workers
- Full schema enforcement for every possible Walrus output line
- Authentication / authorization hardening (later)

---

## 3) High-level architecture (ğŸ¦¦â†’ğŸ¦‰)
- ğŸ¦¦ is spawned per TaskID.
- ğŸ¦¦ emits a stream of **OwlEvent** messages to ğŸ¦‰ (e.g. via `/oscar/event`), including `type=Log`.
- ğŸ¦‰ constructs the â€œmumblingâ€ text stream (MTS, ğŸ¤) by **concatenating `ev.message` from LOG events** (see `SServer::VerboseOut` â†’ `LogLine(ev.message)`).
- ğŸ¦‰ stores:
  - full raw text into per-task log file (debuggable)
  - structured snapshots into TaskRegistry (UI uses these)
- ğŸ¦‰ is an **opportunistic parser**:
  - it listens for known markers already present in logs
  - it accepts partial structure and upgrades state when it can

So, by definition, MTS (ğŸ¤) is a text stream, as recorded by ğŸ¦‰ as described above. It is the canonical human-mumble channel used by the Opportunistic Listener.

---

## 4) ğŸ¦‰ â€œLifecycle Modelâ€ for ğŸ¦¦ (state machine)
Oscar holds a per-task phase; phase guides parsing and timeouts.

### State 0 â€” SPAWNED (technical)
Entry: process created, pipes attached, TaskRegistry entry touched.
Exit: first output line OR immediate process exit.

### State 1 â€” HEADER / INIT (semantic refusal possible)
Typical content: task name, auction, question, fixed hand, scorer, recognized task type.
Failure possible: â€œERROR: â€¦â€, â€œFailed to init â€¦â€

Transitions to FILTERING:
- if we see `Compiling filters...` or other init-success hints

### State 2 â€” FILTERING / SEARCH
Walrus generates random boards and filters by constraints.
This phase can be:
- fast (seconds) OR
- dominant (minutes) OR
- endless (contradictory filters â†’ accepted=0 forever)

Transition to SOLVING:
- if we see `Solving started:` or `Processed: â€¦ total.`

### State 3 â€” SOLVING / AFTERMATH
Walrus runs DD solving on each accepted board and computes stats.
Natural failures are rare; technical stalls/crashes remain.

Transition to FINAL:
- `Verdict:` OR `A split â€¦` OR end timing summary `The search took â€¦`

### State 4 â€” FINAL
Oscar captures the final answer payload.
Exit: worker exits OR grace-timeout triggers.

### State 5 â€” EXITED
Oscar finalizes status:
- DONE if final markers were seen and exit is clean
- FAILED otherwise (with best available error text)

---

## 5) â€œMarkers already thereâ€ (what ğŸ¦‰ listens for)
Oscar should keep a rolling buffer of recent lines (e.g. last 200â€“500) and use regex triggers.
All markers are optional; Oscar uses whatever appears.

### Filter/Search summary markers (Phase 1)
- `Passing (\d+) for double-dummy inspection.`
  - accepted_total = N (boards passing constraints)
- `(\d+) boards are skipped.`
  - skipped = rejected candidates
- `A pick rate is 1 to (\d+)`
  - pick_rate
- `Total iterations = (\d+)`
  - generator_iterations_total
- `The search is done in ...` (seconds or minutes+seconds)
  - filter_time
- `Memory consumed:\s*([0-9]+)K`
  - mem_kb

### Solve progress markers (Phase 2)
Heartbeat:
- `Processed:\s*(\d+)\s*total.`
  - processed

Nearby optional companions (capture if present within a small window around Processed):
- `Chance to make =\s*([0-9.]+)%`
  - chance_make_pct
- `Averages: ...`
  - averages per action/contract (task-dependent)
- `Comparison: favor bidding ([0-9.]+)%.*same ([0-9.]+)%.*favor defending ([0-9.]+)%`
  - comparison tuple
- `A huge match: \+(\d+) IMPs`
  - imps_total
- `HITS COUNT` table
  - store as â€œdiagnostics payloadâ€ (raw block or structured later)

### Final markers
- `Verdict:\s*(.+)`  âœ… best single-line final decision
- `A split .*:`      âœ… signals final aggregation start
- `The search took ...` âœ… final performance summary

### Failure markers
- `^ERROR:` / `Semantics ERROR:` / `Parsing ERROR:`
- `Failed to init ...`
Also use:
- process exit code != 0 as the strongest â€œtechnical failureâ€ signal.

---

## 6) Parsing strategy with *zero changes* to ğŸ¦¦
Oscar uses â€œwindowed block captureâ€ rather than strict BEGIN/END tags.

- HEADER block: from first line until `----` (or until `Passing ...`)
- CONSTRAINTS block: from `----` until `example:` / `Passing ...`
- EXAMPLE block (optional): from `example:` until `Passing ...`
- FILTER_SUMMARY: from `Passing ...` until `Solving started:` or first `Processed:`
- PROGRESS_TICK: anchored at each `Processed:` line; capture a small window of lines before/after to pick up HITS/Chance/Averages/Comparison.
- FINAL: from `Verdict:` or `A split` until EOF/exit

Oscar stores raw blocks in the per-task log file anyway; structured extraction is â€œbest effort.â€

---

## 7) Failure taxonomy (â˜ ï¸) + watchdogs (ğŸºğŸ’“ pulse monitoring)
Oscar maintains:
- `last_output_ms` (any line)
- `last_progress_ms` (Processed heartbeat)
- `phase` (spawn/header/filter/solve/final)
- optionally: `filter_pulse` (if Walrus supports it later)

### 7.1 Failures by phase
**State 0 SPAWNED (technical â˜ ï¸)**
- spawn failure, cannot start process, immediate crash
Action: FAILED(reason=spawn_failed), include OS error + exit code.

**State 1 HEADER / INIT (semantic refusal â˜ ï¸)**
- invalid input (bad PBN, wrong seat, scorer parse/encode fail)
Action: FAILED(reason=user_input or config_error), capture the error block.

**State 2 FILTERING**
Risks:
- contradictory constraints â†’ endless loop with accepted=0 â˜ ï¸
- very tight constraints â†’ long runtime but valid
We want:
- optional â€œfilter pulseâ€ from Walrus (see ğŸºğŸ’“ Feature Request)
Fallback:
- coarse no-output timeout if silent.

**State 3 SOLVING**
Mostly technical:
- crash, deadlock, stall
Action:
- if no output for too long, assume lost â†’ FAILED(reason=worker_silent â˜ ï¸)
- if Processed doesnâ€™t advance for too long â†’ FAILED(reason=solver_stall â˜ ï¸)

**State 4 FINAL**
- saw final markers but no exit â†’ kill after grace timeout â†’ FAILED(reason=finalization_stall) or DONE if final payload complete.

### 7.2 Suggested timeouts (tunable)
- spawn_first_output_timeout: 10â€“20s
- header_timeout: 60s
- filtering_no_output_timeout: 60â€“180s (depends on expected workloads)
- solving_no_output_timeout: 60â€“120s
- solving_no_progress_timeout: longer (e.g. 5â€“10 min) unless you expect frequent Processed
- final_grace_timeout: 10â€“30s

---

## 8) Feature request to ğŸ¦¦ (minimal and â€œless invasive than Aâ€)
We keep Walrus â€œmumbling,â€ but we want one tiny capability:

### FR-1: Filtering signs-of-life (pulse)
Problem: filtering can dominate runtime; if constraints are contradictory, Walrus may loop forever; Oscar needs a way to distinguish â€œstill workingâ€ vs â€œstuckâ€ vs â€œno samples found.â€

Request:
- During FILTERING, Walrus emits **any** periodic human-readable line such as:
  - `Filtering... iters=123456789 accepted=0 elapsed=45s`
  - `Search heartbeat: iters=... accepted=... rateâ‰ˆ...`
- Oscar will parse opportunistically if present; otherwise ignore.

Desired:
- emitted every X seconds (e.g. 5â€“15s) OR every Y iterations
- should appear **before** the final â€œPassing N for double-dummy inspection...â€ line

### FR-2: â€œConnect early enoughâ€
Interpretation:
- Walrus should start emitting output as soon as it starts filter compilation/search.
- If you later move to socket/protobuf/etc, this maps to â€œconnect to Oscar early,â€ but for now ğŸ¤ is fine: *print early, print occasionally.*

#### FR-3: Semantic refusal signature (HEADER â˜ ï¸)
**Goal:** When ğŸ¦¦ refuses a task in **State 1 (HEADER / INIT)** due to semantic/config issues, ğŸ¦‰ wants a stable *one-line* summary to show in UI, while preserving full details in the ğŸ¤ MTS log.

**Oscar-side policy (ğŸ¦‰):**
- UI-facing `ERROR_SUMMARY` is normalized to:
  - `Failed to init task semantics`
- Full diagnostic text remains in the complete ğŸ¦¦ log (ğŸ¤ MTS).  
- This keeps UI clean and avoids leaking internal parser noise, while retaining deep trace for debugging.

**Walrus-side request (ğŸ¦¦): **
- see W-2

**Acceptance criteria:**
- ğŸ¦‰ detects the failure block (`ERROR:`, `Semantics ERROR:`, `Parsing ERROR:`, `Failed to init ...`)
- ğŸ¦‰ stores the full block in the per-task log (ğŸ¤ MTS)
- ğŸ¦‰ sets task status to `FAILED` with `errorSummary = "Failed to init task semantics"`

---

## 9) ğŸ¦‰ implementation plan (testable order)
Each step should be independently verifiable via logs + unit-ish fixtures.

### Phase P0 â€” Supervisor baseline (already mostly done)
- spawn ğŸ¦¦, make sure ğŸ¤ stream works, store per-task logs
- update TaskRegistry status STARTED/IN_PROGRESS
Test: spawn + log file created + `GET /tasks/:id` works.

### Phase P1 â€” Opportunistic parsing v0 (existing markers only)
- implement phase machine (spawn/header/filter/solve/final)
- regex detectors for:
  - `Passing N...`, `Total iterations`, `search time`, `memory`
  - `Processed N total`, `Chance to make`, `Comparison`, `Verdict`, `A split`
- update TaskRegistry snapshot fields:
  - percent (if total known), processed, accepted_total, verdict, final split lines (raw)
Test: replay saved logs â†’ snapshot matches expected.

### Phase P2 â€” Pulse watchdogs (ğŸºğŸ’“)
- maintain last_output_ms / last_progress_ms
- phase-dependent timeouts and failure reasons
Test: simulate silent worker / stalled progress â†’ Oscar fails task cleanly.

### Phase P3 â€” Final answer extraction (nuanced)
- parse final split tables into structured fields:
  - for â€œmake% by HCPâ€ splits
  - for â€œgo A / same / go Bâ€ comparison splits
- preserve â€œHITS COUNTâ€ as raw diagnostic payload (or structured later)
Test: UI shows a compact final + expandable diagnostics.

### Phase P4 â€” Filtering pulse support (when ğŸ¦¦ adds it)
- parse filtering pulses into:
  - elapsed, iters, accepted
- add â€œcontradictory constraintsâ€ refusal:
  - accepted=0 beyond threshold â†’ fail(reason=no_samples_found)
Test: run a known-contradictory task; Oscar fails fast with meaningful message.

### Phase P5 â€” Better contracts (optional later)
- `resultJson` schema stabilized per task type
- JSONL or @EV upgrade path if/when desired

---

## 10) Split into tasks (Oscar-side planned work)
You can paste these into TTL or issue tracker.

### Core tasks (must)
**T1. Phase machine + marker recognizers**
- Implement lifecycle phases and transitions driven by existing markers.

**T2. Filter summary extractor**
- Extract accepted_total, skipped, pick_rate, iterations, filter_time, mem_kb.

**T3. Progress tick extractor**
- Anchor on `Processed:` and parse chance/averages/comparison if nearby.

**T4. Final answer extractor**
- Detect final markers and assemble final payload:
  - verdict (if present)
  - split tables (raw + structured)
  - perf summary (search sec + aftermath + throughput)

**T5. Pulse watchdogs (ğŸºğŸ’“)**
- last_output_ms / last_progress_ms + phase timeouts + failure reasons.

### Integration tasks (near-term, highly recommended)
**T6. TaskRegistry as the single source of truth**
- Ensure dedupe/seq handling and â€œlast seenâ€ live only in TaskRegistry (encapsulation refactor).

**T7. Log plumbing + retrieval**
- Per-task logs reliable and separated
- Optional: `?log_req=1` support for UI retrieval (tail)

### Walrus-facing tasks (feature requests)
**W1. Filtering pulse line**
- Add â€œsign of lifeâ€ printing during filtering.

**W2. Semantic refusal signature**
- On semantic/config refusal in HEADER â˜ ï¸, emit to ğŸ¤ keyword â€œERRORâ€ and/or â€œFailedâ€. 
  - Avoid emitting these keywords on normal workflow.
- Continue emitting the detailed failure messages as usual (no need to restructure them).

---

## Appendix A â€” Other ğŸ¦‰ tasks & optional features
### A1) Team Task List (TTL) items weâ€™re carrying
- **Parser rewrite (future): `OwlEvent::AttemptParse`**  
  Rewrite to support richer event schema when/if we move to structured events (log/progress/final/fail + resultJson + diagnostics).  
  DoD: fail events produce FAILED; final result payload flows to UI; tests with fixtures.

- **TaskRegistry encapsulates old `tasks[]` map**  
  Move dedupe/seq filtering entirely into TaskRegistry; Oscar uses returned disposition (`Accepted/Duplicate/Ignored`).  
  (This reduces drift/leaks and makes the listener architecture cleaner.)

- **Optional: DELETE semantics alignment**
  Ensure `mode=cancel` vs `mode=ack` behavior matches comments and expectations.

- **Optional: global log newline policy**
  Decide whether server log should force newline separation (per-task logs already do it well).

### A2) Optional feature ideas (nice-to-have)
- â€œDiagnostics packâ€ in UI: show split tables + HITS COUNT + perf
- Rate limit protection: cut off flooding workers per task (you already want this)
- Structured â€œtask familyâ€ display: competitive compare vs one-sided make vs others
- Auto-sweep policy: once UI ACKs (`DELETE mode=ack`), registry may evict snapshot + delete task log file

### A3) Collaboration flags (ğŸŸ©ğŸŸ¨ğŸŸ¥)
- ğŸŸ© Go: implement a phase or extractor with tests against saved logs
- ğŸŸ¨ Slow: changes that affect timeouts/kill policies; check assumptions
- ğŸŸ¥ Stop: any ambiguity in TaskID / ownership / deleting active tasks requires clarity

---

## Appendix B â€” â€œUpgrade pathâ€ (if we ever want)
Even though we start with mumble-parsing, the doc keeps a clean ladder:
- v0.1: parse existing markers
- v0.2: add filtering pulses
- v0.3: optional single-line embedded JSON events (`@EV {...}`)
- v0.4: optional full JSONL event stream

We do not need to decide now. The opportunistic listener approach stays valid and useful across all versions.

---

## APPENDIX C â€” Phase transition triggers
Request a separate file from Architect.

